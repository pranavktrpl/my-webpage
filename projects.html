<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Pranav - Projects</title>

  <!-- GitHub Pages path handling -->
  <script>
    // Function to get the correct base path for GitHub Pages
    function getBasePath() {
      const isGitHubPages = window.location.hostname.includes('github.io');
      return isGitHubPages ? '/my-webpage' : '';
    }
    
    // Create a variable we can use in our CSS link
    window.basePath = getBasePath();
    document.write('<link rel="stylesheet" href="' + window.basePath + '/css/styles.css" />');
  </script>

  <!-- Load Lora (for main body & headings) from Google Fonts -->
  <link
    href="https://fonts.googleapis.com/css2?family=Lora:wght@400;700&display=swap"
    rel="stylesheet"
  />

  <!-- Load Aileron from cdnfonts.com (for nav links & subhead) -->
  <link
    rel="stylesheet"
    href="https://fonts.cdnfonts.com/css/aileron"
  />
</head>
<body>
  
  <!-- Include Header -->
  <div data-include="header.html"></div>
  
   
<!-- Projects Section -->
<section id="projects">
    <h2>Projects</h2>

    <h2>Currently working on (as of 27th January 2026)</h2>
    <!-- Project Entry: LeJEPA on Galaxy Images (In Progress) -->
    <div class="project-entry">
      <h3>LeJEPA-Style Self-Supervised Model for Galaxy Representations</h3>
      <p class="project-subtitle">
        <em>UniverseTBD | Self-Supervised Vision | In Progress (Jan 2026 – Present)</em>
      </p>
      <ul class="project-details">
        <li>Training a JEPA-style self-supervised model on ~8.5M DESI Legacy Survey DR8 galaxy cutouts (512×512), with fixed train/val/test splits.</li>
        <li>Using the same underlying image distribution as our AstroLLaVA work, aiming to compare what a JEPA vs a fine-tuned vision-language model learns about astrophysical structure.</li>
        <li>Early-stage project focused on representation probing and objective-driven differences; targeting a research paper submission by end of March / April 2026.</li>
      </ul>
    </div>

    <!-- Project Entry: Wunder Fund Challenge 2 (In Progress) -->
    <div class="project-entry">
      <h3>Wunder Challenge 2 — Limit Order Book Forecasting</h3>
      <p class="project-subtitle">
        <em>ML Competition | High-Frequency Time-Series | In Progress (Jan 2026 – Present)</em>
      </p>
      <ul class="project-details">
        <li>Working on a sequence model to predict two future price-movement targets (t0, t1) from anonymized LOB + recent trades as a long horizon forecasting task.</li>
        <li>Explored Transformers, RNN/GRU/LSTM baselines, and kNN-style methods; scored above the vanilla baseline early and am currently iterating on state-space models for further gains.</li>
      </ul>
    </div>

    <!-- WorldQuant Project 3: Macro Risk Exposure Taxonomy (Exploratory) -->
    <div class="project-entry">
      <h3>WorldQuant — Macro Risk Exposure Taxonomy from Filings (Exploratory)</h3>
      <p class="project-subtitle">
        <em>WorldQuant | Taxonomy + Prompt Schemas | Jan 2026 - Present
          &nbsp;•&nbsp;
          <a href="assets/Pranav_Khetarpal_WorldQuant_Internship_Summary.pdf" target="_blank" rel="noopener">
            Internship memo (PDF)
          </a>
        </em>
      </p>
      <ul class="project-details">
        <li>Initiated a second-phase project at Worldquant but was unable to complete it during my internship due to limited time.</li>
        <li>Working on ontology, and building the pipeline to quantify firm-level macro risk exposures from filings via an LLM-driven taxonomy and scoring rubric.</li>
        <li>Drafted early factor definitions and prompt/schema scaffolding to support consistent extraction and PIT-safe aggregation.</li>
      </ul>
    </div>

    <h2>Quant & Markets</h2>

    <div id="worldquant">    

      <!-- WorldQuant Project 1: Relationship Graph (Filings + Sell-side + Ontology) -->
      <div class="project-entry">
        <h3>WorldQuant — PIT Relationship Graph Alpha (SEC Filings + Sell-Side Reports)</h3>
        <p class="project-subtitle">
          <em>
            Quantitative Research Internship | DeepHuman Algo Lab (DeepResearch) | Sep–Dec 2025
            &nbsp;•&nbsp;
            <a href="assets/Pranav_Khetarpal_WorldQuant_Internship_Summary.pdf" target="_blank" rel="noopener">
              Internship memo (PDF)
            </a>
          </em>
        </p>
        <ul class="project-details">
          <li>Built point-in-time (PIT) pipelines to extract inter-company relationships from 10-K/10-Q SEC Filings (MD&amp;A + Notes sec.) and OCR’d sell-side analyst reports using regex, structured LLM prompting + schema-constrained outputs.</li>
          <li>Produced directed edge tables with relation classes (supply chain/customer, competitor, litigation, regulatory, M&amp;A, macro/other) and directional AB/BA sentiment signals.</li>
          <li>Defined an 11-factor edge-weighting ontology (10 directional + 1 direction-invariant) to quantify relationship strength, polarity, and asymmetry—standardized across both data sources.</li>
          <li>Scaled across 2020–2024 (12k filings, ~400k reports) with QC + provenance fields in under 300USD credits each; packaged graph outputs as backtest-ready tables <strong>(indicative internal L/S-neutral results under stated assumptions = ~1.5-1.6 Sharpe from filings, ~2.2-2.3 from analyst reports).</strong></li>
        </ul>
      </div>

      <!-- WorldQuant Project 2: UPC Shipments Signals -->
      <div class="project-entry">
        <h3>WorldQuant — UPC Shipments Alternative-Data Signals (Issuer-Level Daily Features)</h3>
        <p class="project-subtitle">
          <em>WQ | Aggregation + QC | Sep–Dec 2025
            &nbsp;•&nbsp;
            <a href="assets/Pranav_Khetarpal_WorldQuant_Internship_Summary.pdf" target="_blank" rel="noopener">
              Internship memo (PDF)
            </a>
          </em>
        </p>
        <ul class="project-details">
          <li>Built issuer-level daily feature tables from UPC shipment streams with point-in-time (PIT) alignment and robust handling of missingness, returns, discontinuations, and vendor breaks.</li>
          <li>Implemented time-varying UPC weighting &amp; gating using quality/stability signals (e.g., value-share, consistency, co-movement), plus backfills/imputations to stabilize noisy series.</li>
          <li>Resolved major data-quality and regime issues (corrupted histories, inconsistent mappings, vendor/coverage shifts) with explicit QC flags, repair logic, and audit-friendly provenance.</li>
          <li>Ran a battery of pre-backtest diagnostics (SNR/strength measures, autocorrelation &amp; stability checks, coverage/turnover analysis, distribution-shift and PIT leakage/sanity checks) to validate signal behavior before downstream backtesting.</li>        
          <li>Delivered backtest-ready aggregates and monitoring/QC fields for downstream research <strong>(indicative internal L/S-neutral results under stated assumptions = ~1.3–1.4 Sharpe).</strong></li>
        </ul>
      </div>

    </div>

    <div class="project-entry">
      <h3>NIFTY 50 Implied-Volatility Reconstruction Challenge</h3>
      <p class="project-subtitle">
        <em>Kaggle Competition | Ranked Top∼75 out of 1500+ participants globally</em>
      </p>
      <ul class="project-details">
        <li>Built a real-time engine that infers missing per-second call/put IVs for every strike using only the spot price, 42 engineered micro-structure factors, and any partial IV quotes.</li>
        <li>Bench-tested a spectrum of gap-filling strategies, log-moneyness cubic fits, SVI, linear X →IV maps, and PCA, LightGBM, MLP ensembles quantitatively comparing each step and blending the best.</li>
        <li>Delivered a 50 × RMSE drop over a “heavy-model-only” baseline while preserving no-arbitrage shape and beat the competition's private-leaderboard benchmark RMSE, demonstrating robust generalization to shuffled timestamps and noisy market micro-structure.</li>
      </ul>
    </div>
    
    <!-- Project Entry 5 -->
    <div class="project-entry">
      <h3>Monte Carlo Pricing of Temperature Weather Derivatives</h3>
      <p class="project-subtitle">
        <em>Prof. Manabendra Saharia | Course Project</em>
      </p>
      <ul class="project-details">
        <li>Developed a robust methodology for pricing temperature-based weather derivatives using the Ornstein-Uhlenbeck process combined with Monte Carlo simulations.</li>
        <li>Designed and calibrated models to capture trends, seasonality, and stochastic volatility in temperature data for accurate derivative pricing.</li>
        <li>Conducted risk-neutral analysis and simulation-based pricing for Heating Degree Day (HDD) options, enabling risk management in weather-sensitive industries.</li>
        <li>Validated Monte Carlo simulation results against the Black-Scholes model, ensuring accuracy and reliability in derivative pricing.</li>
      </ul>
    </div>
    


    <h2>AI/ML Research (LLMs & Multimodal) (publications + projects)</h2>
    <!-- Project Entry (REPLACE): MaCBench -->
    <div class="project-entry">
      <h3>MaCBench — Multimodal Benchmark for Chemistry &amp; Materials Research</h3>
      <p class="project-subtitle">
        <em>
          M3RG Lab |
          Workshop (NeurIPS AIforMat Spotlight):
          <a href="https://openreview.net/forum?id=Q2PNocDcp6" target="_blank" rel="noopener">OpenReview</a>
          |
          Extended version:
          <a href="https://arxiv.org/abs/2411.16955" target="_blank" rel="noopener">arXiv:2411.16955</a>
          |
          Journal:
          <a href="https://www.nature.com/articles/s43588-025-00836-3" target="_blank" rel="noopener">Nature Computational Science</a>
        </em>
      </p>
      <ul class="project-details">
        <li>Co-developed MaCBench, a real-world benchmark spanning data extraction, experimental understanding, and results interpretation for chemistry/materials workflows.</li>
        <li>Ran systematic evaluations of frontier multimodal models and characterized failure modes beyond basic perception—especially spatial reasoning, cross-modal synthesis, and multi-step inference.</li>
        <li>Released the initial workshop spotlight paper and contributed to the expanded evaluation + analysis in the extended/journal version.</li>
      </ul>
    </div>
    <div class="project-entry">
      <h3>MaCBench — Multimodal Benchmark for Chemistry &amp; Materials Research</h3>
      <p class="project-subtitle">
        <em>
          M3RG Lab |
          Workshop (NeurIPS AIforMat Spotlight):
          <a href="https://openreview.net/forum?id=Q2PNocDcp6" target="_blank" rel="noopener">OpenReview</a>
          |
          Extended version:
          <a href="https://arxiv.org/abs/2411.16955" target="_blank" rel="noopener">arXiv:2411.16955</a>
          |
          Journal:
          <a href="https://www.nature.com/articles/s43588-025-00836-3" target="_blank" rel="noopener">Nature Computational Science</a>
        </em>
      </p>
      <ul class="project-details">
        <li>Co-developed MaCBench, a real-world benchmark spanning data extraction, experimental understanding, and results interpretation for chemistry/materials workflows.</li>
        <li>Ran systematic evaluations of frontier multimodal models and characterized failure modes beyond basic perception—especially spatial reasoning, cross-modal synthesis, and multi-step inference.</li>
        <li>Released the initial workshop spotlight paper and contributed to the expanded evaluation + analysis in the extended/journal version.</li>
      </ul>
    </div>
        
    <!-- Project Entry 2 -->
    <div class="project-entry">
      <h3>Knowledge Base Effort</h3>
      <p class="project-subtitle">
        <em>Prof. N.M. Anoop Krishnan & Prof. Mausam | M3RG Lab | GitHub: <a href="https://github.com/pranavktrpl/complete-llamat-pipeline" rel="noopener">Repo</href></em>
      </p>
      <ul class="project-details">
        <li>Developed a knowledge base and data extraction model to aggregate material science data from scientific literature, forming a foundation for domain-specific large language models.</li>
        <li>Focused on advanced entity linkage and extracting scientific information such as chemical formulas to enhance data accessibility and usability.</li>
        <li>Built an end-to-end, multi-agent pipeline that ingests publisher XML's of research papers, extracts structured text & tables, mines chemical compositions, and contextually links them to material property queries with evidence & confidence scoring.</li>
        <li>Productionized a fine-tuned 8B LLaMAT model (local, GPU-aware, low-cost) with domain-adapted prompts, scientific sentence chunking and table classification that feed a growing materials knowledge base.</li>
      </ul>
    </div>


    <!-- Project Entry: SmolMoE -->
    <div class="project-entry">
      <h3>SmolMoE — Mixture-of-Experts Transformer Language Model (Upcycling + Continued Pretraining)</h3>
      <p class="project-subtitle">
        <em> Cohere Labs — Scholar Take-Home</em>
      </p>
      <ul class="project-details">
        <li>Implemented a compact decoder-only language model with Mixture-of-Experts (MoE) feed-forward blocks, integrating routing logic and MoE-aware training components.</li>
        <li>Added MoE observability and routing-health monitoring: expert utilization / load distribution tracking, plus a routing-specialization style metric to quantify how selectively experts are used.</li>
        <li>Built an <strong>upcycling</strong> path to convert a dense Transformer into an MoE model by copying backbone weights and initializing expert banks from the dense MLP, enabling continued-pretraining from a dense checkpoint.</li>
      </ul>
    </div>

    
    <!-- Project Entry: AstroLLaVA -->
    <div class="project-entry">
      <h3>AstroLLaVA — Astronomy Vision-Language Model</h3>
      <p class="project-subtitle">
        <em>
          UniverseTBD | Paper:
          <a href="https://arxiv.org/abs/2504.08583" target="_blank" rel="noopener">arXiv:2504.08583</a>
        </em>
      </p>
      <ul class="project-details">
        <li>Contributed to evaluation + deployment workflows for AstroLLaVA, a domain-adapted vision-language model for astronomy built on the LLaVA stack.</li>
        <li>Supported experimental comparisons on astronomy image–text tasks and helped harden the pipeline for reproducible runs and model release.</li>
        <li>Project output includes curated astronomy visual QA resources and benchmarking for astronomy-focused multimodal reasoning.</li>
      </ul>
    </div>

    
    <!-- Project Entry 3 -->
    <div class="project-entry">
      <h3>Meta-Agentic Retrieval-Augmented Generation System</h3>
      <p class="project-subtitle">
        <em>Hackathon | Inter-IIT Technical Meet, 2024</em>
      </p>
      <ul class="project-details">
        <li>Developed a platform integrating Retrieval-Augmented Generation (RAG) with dynamic knowledge graph creation, enabling real-time data retrieval from diverse sources like academic papers, web content, and live news.</li>
        <li>Designed a meta-agent for autonomous optimization, agent creation, and feedback incorporation, enhancing adaptability and accuracy.</li>
      </ul>
    </div>

    <!-- Project Entry: Trexquant (Hangman / Joining Test) -->
    <div class="project-entry">
      <h3>Hangman Solver — Multi-Model Ensemble</h3>
      <p class="project-subtitle">
        <em>Trexquant Quant Hiring Challenge | Sequence Models, Boosted Trees & N-gram LMs</em>
      </p>
      <ul class="project-details">
        <li><strong>Models explored:</strong> baseline heuristic, BiLSTM consonant predictor, BiLSTM-Attention, LightGBM (26 binary letter classifiers), 1–5 gram frequency tables, static neural–symbolic blends, temperature-scaled fusion, candidate-pruning inference, and an MLP meta-learner (abandoned).</li>
    
        <li><strong>System design:</strong> length-aware vowel priors, masked-state modeling, calibrated probability fusion, fast n-gram lookup, and constraint-based candidate filtering.</li>
    
        <li><strong>Top results:</strong>
          Offline evaluation peaked at <strong>0.81</strong> win-rate (BiLSTM-Attn + calibrated blend + pruning).
          Public server runs achieved <strong>0.646</strong> mean win-rate (3×1000 games) vs ~18% baseline.
        </li>
      </ul>
    </div>
        
    <!-- Project Entry 4 -->
    <div class="project-entry">
      <h3>AstroLLaMA</h3>
      <p class="project-subtitle">
        <em>Dr. Ioana Ciuca | UniverseTBD</em>
      </p>
      <ul class="project-details">
        <li>Co-developed AstroLLaMA, a 7-billion-parameter language model fine-tuned from LLaMA-2 using over 300,000 astronomy abstracts from arXiv, achieving a 30% reduction in perplexity compared to LLaMA-2.</li>
        <li>Facilitated the public release of AstroLLaMA to promote astronomy-focused research, including applications in automatic paper summarization and the development of conversational agents.</li>
      </ul>
    </div>

    <!-- Project Entry 7 -->
    <div class="project-entry">
      <h3>AstroTalks</h3>
      <p class="project-subtitle">
        <em>Dr. David Hendriks | UniverseTBD</em>
      </p>
      <ul class="project-details">
        <li>Developed a program leveraging OpenAI's Whisper AI tool to transcribe video datasets accurately.</li>
        <li>Collaborated with NASA ADS to create and integrate LLM-based pipelines for real-time transcript extraction, topic modeling, and periodic model retraining, enhancing platform performance and accessibility.</li>
      </ul>
    </div>

    <h2>Foundations and Engineering (Coursework / Core CS)</h2>

    <!-- Coursework Project: Deep Learning for NLP -->
    <div class="project-entry">
      <h3>Deep Learning for NLP — Language Modeling & Structured Prediction</h3>
      <p class="project-subtitle">
        <em>Prof. Tanmoy Chakraborty | Course Projects</em>
      </p>
      <ul class="project-details">
        <li>Built up-to-5-gram language models and a noisy-channel spell-correction pipeline (candidate generation + LM scoring) for robust correction under realistic noise.</li>
        <li>Implemented a linear-chain CRF from scratch in NumPy for sequence tagging, including feature design and Viterbi decoding.</li>
      </ul>
    </div>
        
    <!-- Project Entry 6 -->
    <div class="project-entry">
      <h3>Implementation and Applications of Machine Learning Algorithms</h3>
      <p class="project-subtitle">
        <em>Prof. Rahul Garg | Course Projects</em>
      </p>
      <ul class="project-details">
        <li><strong>Implemented Core ML Algorithms:</strong> Built machine learning models from scratch, including Linear Regression, Ridge Regression, Logistic Regression, and feature selection methods such as PCA and ANOVA. Applied advanced feature engineering techniques to optimize model performance.</li>
        <li><strong>Deep Learning Architectures:</strong> Developed a 5-layer feed-forward neural network with backpropagation and a 6-layer convolutional neural network (CNN), fully implemented from scratch, to explore neural network design and training dynamics.</li>
        <li><strong>High-Confidence Predictions Project:</strong> Utilized PyramidNet with SAM (Sharpness-Aware Minimization) optimization on the CIFAR-100 dataset to achieve high-confidence predictions or allow the model to abstain when uncertain.</li>
        <li><strong>Advanced Classifiers:</strong> Achieved the highest accuracy in class by implementing a 6-class Naive Bayes classifier (Bernoulli and Multinomial) on the LIAR dataset and developed an Oblique Decision Tree with pruning based on validation set performance.</li>
      </ul>
    </div>
        
    <!-- Project Entry 8 -->
    <div class="project-entry">
      <h3>Implementation and Use of Data Structures & Algorithms</h3>
      <p class="project-subtitle">
        <em>Prof. Keerti Choudhary | Course Projects</em>
      </p>
      <ul class="project-details">
        <li>Developed core data structures and algorithms from scratch, encompassing Stacks, Linked Lists, Dictionaries, Hashmaps, Skip Lists, Binary Trees, BSTs, AVL Trees, 2-4 Trees, Heaps, Graphs, etc., and implemented Dijkstra's, BFS, and DFS algorithms in Java during a comprehensive DSA course.</li>
        <li>Utilized DFS and other graph algorithms to find the most optimal moves in Snakes and Ladders game. Analyzed the impact of adding a snake/ladder between nodes on game outcomes and identified strategic placements for improved odds.</li>
        <li>Used Decision Trees and Mini-Max algorithm to play Othello.</li>
      </ul>
    </div>
    
    <!-- Project Entry 9 -->
    <div class="project-entry">
      <h3>Truss Analysis</h3>
      <p class="project-subtitle">
        <em>Prof. N.M. Anoop Krishnan | Course Project</em>
      </p>
      <ul class="project-details">
        <li>Built Python-based truss analysis software employing Numpy and Matplotlib libraries for comprehensive structural analysis using FEM, solving equations for displacements, stresses, and visualizing results.</li>
      </ul>
    </div>
    
  </section>
  
  <!-- Include Footer -->
  <div data-include="footer.html"></div>

  <!-- Include Header and Footer JavaScript -->
  <script src="js/header.js"></script>
</body>
</html>
